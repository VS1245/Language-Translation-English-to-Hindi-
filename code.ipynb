{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELF-ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.75249785e-01,  4.83159148e-01,  4.66497395e-01,\n",
       "         -2.37673038e+00,  6.18552254e-01,  1.53266532e+00,\n",
       "         -1.04888246e+00, -1.62380375e-03],\n",
       "        [-8.26393921e-02, -2.22246045e-02, -6.18850687e-01,\n",
       "          1.59509308e+00, -2.98899854e-01,  4.70837132e-01,\n",
       "         -8.93437088e-01,  3.08799844e-01],\n",
       "        [ 7.85949434e-01,  4.61509339e-01,  1.37698676e+00,\n",
       "         -3.73440967e-01,  4.68181686e-01, -8.24196672e-01,\n",
       "          9.93433622e-01, -1.34819707e+00],\n",
       "        [ 3.23789993e-01,  2.84369911e+00, -9.19288295e-01,\n",
       "          1.41435015e+00, -8.06813482e-01, -2.25586791e-02,\n",
       "          5.54345963e-01, -8.68484336e-01]]),\n",
       " array([[-0.55363748,  1.51504698,  0.48155705, -0.46345492, -1.5538429 ,\n",
       "         -0.97751189, -0.66632258,  0.12906853],\n",
       "        [ 1.21471272,  0.52868581, -0.74968218, -1.38319245, -0.71161034,\n",
       "         -0.44771393,  0.88902941,  0.64562888],\n",
       "        [-0.95525258,  0.62577711,  0.03178567,  1.06402966, -1.04426736,\n",
       "          0.8527665 , -0.68867453,  0.10337687],\n",
       "        [ 2.42799988,  1.56941454, -0.73774612,  0.10780127, -0.63921831,\n",
       "         -1.00376657, -0.96633636,  0.5194749 ]]),\n",
       " array([[ 0.12001635, -0.31246027,  0.16569375,  2.72159384,  1.04951631,\n",
       "         -0.18897771,  1.62848842, -1.92552051],\n",
       "        [-0.16454998,  0.33019786, -0.83968804, -1.16813863, -0.12978502,\n",
       "          0.11638485,  1.13043326,  1.83998588],\n",
       "        [-1.30327901, -0.75494127, -0.29897515,  1.91466055, -0.02298287,\n",
       "         -0.78431215,  0.61658648, -0.46009354],\n",
       "        [-1.58981452, -2.18336681, -0.43780635, -0.69882462,  2.10023833,\n",
       "         -0.89777911,  0.14836206,  1.83273344]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4,8,8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "q,k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.1451256 ,  1.46763999, -1.09142433, -0.09488614],\n",
       "        [-0.38581654, -2.44753419,  3.10344523,  1.1352059 ],\n",
       "        [ 0.3424722 ,  0.73153265, -2.83084106,  0.44414574],\n",
       "        [ 3.8251437 ,  1.14593875,  3.29765745,  5.631308  ]]),\n",
       " 4.677161490285577)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T) ,np.matmul(q,k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5846451862856971"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MASKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05130965,        -inf,        -inf,        -inf],\n",
       "       [-0.13640675, -0.86533401,        -inf,        -inf],\n",
       "       [ 0.12108221,  0.25863585, -1.00085345,        -inf],\n",
       "       [ 1.35239253,  0.40515053,  1.16589797,  1.99096804]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05264879  1.29339862  2.79144538 15.89761569]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.05264879, 0.        , 0.        , 0.        ],\n",
       "       [0.87248768, 0.42091094, 0.        , 0.        ],\n",
       "       [1.1287177 , 1.29516208, 0.36756561, 0.        ],\n",
       "       [3.86666557, 1.49952821, 3.208803  , 7.32261891]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(np.exp(scaled + mask),axis = -1))\n",
    "(np.exp(scaled + mask)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x),axis = -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.67456982, 0.32543018, 0.        , 0.        ],\n",
       "       [0.40434884, 0.46397543, 0.13167573, 0.        ],\n",
       "       [0.24322299, 0.09432409, 0.20184178, 0.46061114]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12001635, -0.31246027,  0.16569375,  2.72159384,  1.04951631,\n",
       "        -0.18897771,  1.62848842, -1.92552051],\n",
       "       [ 0.02740988, -0.10331992, -0.16148782,  1.45575751,  0.66573607,\n",
       "        -0.08960352,  1.46640624, -0.7001111 ],\n",
       "       [-0.19942889, -0.07254669, -0.36196432,  0.8106    ,  0.36112735,\n",
       "        -0.12568808,  1.26416013,  0.01454312],\n",
       "       [-0.98167272, -1.20291367, -0.30090644,  0.61634265,  1.2057789 ,\n",
       "        -0.60681985,  0.69550304,  0.4565355 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.12001635, -0.31246027,  0.16569375,  2.72159384,  1.04951631,\n",
       "         -0.18897771,  1.62848842, -1.92552051],\n",
       "        [ 0.02740988, -0.10331992, -0.16148782,  1.45575751,  0.66573607,\n",
       "         -0.08960352,  1.46640624, -0.7001111 ],\n",
       "        [-0.19942889, -0.07254669, -0.36196432,  0.8106    ,  0.36112735,\n",
       "         -0.12568808,  1.26416013,  0.01454312],\n",
       "        [-0.98167272, -1.20291367, -0.30090644,  0.61634265,  1.2057789 ,\n",
       "         -0.60681985,  0.69550304,  0.4565355 ]]),\n",
       " array([[1.        , 0.        , 0.        , 0.        ],\n",
       "        [0.67456982, 0.32543018, 0.        , 0.        ],\n",
       "        [0.40434884, 0.46397543, 0.13167573, 0.        ],\n",
       "        [0.24322299, 0.09432409, 0.20184178, 0.46061114]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(q,k,v,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn((batch_size, L, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, L, num_heads, 3*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0852,  0.1322,  0.2466,  ...,  1.0625, -0.2707,  0.3686],\n",
       "          [-0.0132, -1.4978, -0.9129,  ...,  0.9324,  0.4472,  0.8599],\n",
       "          [ 0.1751, -0.0577, -0.0960,  ...,  1.2166, -0.3852, -0.4915],\n",
       "          [ 0.5527, -0.1625, -0.5184,  ...,  0.6852, -0.9270,  0.3845]],\n",
       "\n",
       "         [[ 0.8466, -0.6219,  0.3334,  ...,  0.4208, -0.0808, -1.0024],\n",
       "          [ 0.8024,  0.5604, -0.6831,  ..., -0.6170, -0.2889,  0.9852],\n",
       "          [ 0.8055, -0.3160,  1.5276,  ..., -0.0138, -0.6528,  0.4051],\n",
       "          [-0.2784,  0.3245, -0.6296,  ..., -0.6623, -0.3197,  0.8186]],\n",
       "\n",
       "         [[-0.6355,  0.2266, -0.3077,  ...,  0.1214, -0.6224, -0.7657],\n",
       "          [ 0.9627,  1.3713, -0.4762,  ..., -0.4577, -0.0143,  0.1391],\n",
       "          [-1.0184,  0.1357,  0.5903,  ...,  0.1498, -0.4999, -0.0425],\n",
       "          [ 0.1297,  0.2458,  0.2871,  ...,  0.2909,  0.5753, -0.4591]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5632, -0.3538,  0.4080,  ...,  0.2881, -1.4450, -0.0399],\n",
       "          [-0.4279,  0.7804, -0.2991,  ...,  0.8284, -0.1895,  0.4956],\n",
       "          [ 0.1175, -0.4426,  0.4601,  ...,  0.1381,  0.5887,  0.4482],\n",
       "          [-0.3872, -0.0325, -0.2271,  ...,  0.2657,  0.3787,  0.0092]],\n",
       "\n",
       "         [[ 0.2879,  0.8002,  0.2171,  ...,  0.5600,  0.4704,  0.4140],\n",
       "          [ 0.4493,  0.6536,  0.0248,  ..., -0.6770, -0.2845, -0.1904],\n",
       "          [ 0.0919, -1.0069,  0.8275,  ..., -0.3307, -0.5495, -0.1311],\n",
       "          [ 0.8239, -0.0095,  0.0095,  ...,  0.3232, -0.0341,  0.9699]],\n",
       "\n",
       "         [[-1.1424, -0.5038,  0.8494,  ..., -1.0867, -0.4793,  0.7211],\n",
       "          [-0.1381, -0.2676,  0.0165,  ...,  0.0603,  0.0803, -0.0337],\n",
       "          [-0.7458, -0.0822,  0.6263,  ..., -0.1261, -0.0181, -0.8270],\n",
       "          [ 0.7042,  0.3614, -0.0463,  ..., -0.1830, -0.3810,  0.0783]]]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3)\n",
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-2,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf],\n",
       "          [0., 0., 0., -inf],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0735,    -inf,    -inf,    -inf],\n",
       "          [ 0.0782,  0.2059,    -inf,    -inf],\n",
       "          [-0.5030, -0.0269, -0.2396,    -inf],\n",
       "          [-0.4872, -0.2374,  0.1777,  0.1478]],\n",
       "\n",
       "         [[-0.3169,    -inf,    -inf,    -inf],\n",
       "          [-0.2388,  0.0253,    -inf,    -inf],\n",
       "          [-0.2687, -0.1476,  0.1627,    -inf],\n",
       "          [-0.0392, -0.2156,  0.0171, -0.4682]],\n",
       "\n",
       "         [[ 0.1863,    -inf,    -inf,    -inf],\n",
       "          [-0.1263,  0.5383,    -inf,    -inf],\n",
       "          [ 0.3032,  0.9043,  0.1395,    -inf],\n",
       "          [ 0.2131,  0.4090, -0.2700,  0.1084]],\n",
       "\n",
       "         [[ 0.1788,    -inf,    -inf,    -inf],\n",
       "          [ 0.3041, -0.2295,    -inf,    -inf],\n",
       "          [-0.0592,  0.0471, -0.2240,    -inf],\n",
       "          [-0.0369,  0.2298,  0.1391, -0.0552]],\n",
       "\n",
       "         [[-0.0401,    -inf,    -inf,    -inf],\n",
       "          [ 0.1712,  0.1196,    -inf,    -inf],\n",
       "          [-0.6292,  0.2859,  0.5819,    -inf],\n",
       "          [ 0.3516, -0.2441,  0.5693, -0.1603]],\n",
       "\n",
       "         [[-0.4056,    -inf,    -inf,    -inf],\n",
       "          [ 0.0933, -0.4217,    -inf,    -inf],\n",
       "          [ 0.0242,  0.3263,  0.4543,    -inf],\n",
       "          [-0.1988, -0.3800, -0.2853, -0.0325]],\n",
       "\n",
       "         [[ 0.3022,    -inf,    -inf,    -inf],\n",
       "          [ 0.4413, -0.2976,    -inf,    -inf],\n",
       "          [ 0.2381,  0.2554,  0.4224,    -inf],\n",
       "          [ 0.0772, -0.1952,  0.0828, -0.0106]],\n",
       "\n",
       "         [[ 0.2294,    -inf,    -inf,    -inf],\n",
       "          [-0.2449, -0.3184,    -inf,    -inf],\n",
       "          [ 0.0536, -0.2104, -0.0847,    -inf],\n",
       "          [-0.2125, -0.2339,  0.2906, -0.1482]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+=mask\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4681, 0.5319, 0.0000, 0.0000],\n",
       "          [0.2557, 0.4116, 0.3327, 0.0000],\n",
       "          [0.1635, 0.2099, 0.3179, 0.3086]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4343, 0.5657, 0.0000, 0.0000],\n",
       "          [0.2726, 0.3077, 0.4197, 0.0000],\n",
       "          [0.2819, 0.2363, 0.2982, 0.1836]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3397, 0.6603, 0.0000, 0.0000],\n",
       "          [0.2722, 0.4966, 0.2312, 0.0000],\n",
       "          [0.2678, 0.3258, 0.1652, 0.2412]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6303, 0.3697, 0.0000, 0.0000],\n",
       "          [0.3378, 0.3757, 0.2865, 0.0000],\n",
       "          [0.2232, 0.2914, 0.2662, 0.2192]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5129, 0.4871, 0.0000, 0.0000],\n",
       "          [0.1459, 0.3643, 0.4898, 0.0000],\n",
       "          [0.2947, 0.1624, 0.3663, 0.1766]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6260, 0.3740, 0.0000, 0.0000],\n",
       "          [0.2571, 0.3477, 0.3952, 0.0000],\n",
       "          [0.2543, 0.2122, 0.2332, 0.3003]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6768, 0.3232, 0.0000, 0.0000],\n",
       "          [0.3106, 0.3160, 0.3734, 0.0000],\n",
       "          [0.2715, 0.2068, 0.2730, 0.2487]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5184, 0.4816, 0.0000, 0.0000],\n",
       "          [0.3790, 0.2910, 0.3300, 0.0000],\n",
       "          [0.2128, 0.2083, 0.3519, 0.2269]]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim=-1)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4777,  0.3902,  1.4209,  ...,  1.0625, -0.2707,  0.3686],\n",
       "          [-0.4544, -0.2658, -0.0801,  ...,  0.9933,  0.1112,  0.6299],\n",
       "          [-0.4466, -0.4839, -0.3586,  ...,  1.0602, -0.0133,  0.2846],\n",
       "          [-0.2370, -0.3296, -0.4426,  ...,  0.9678, -0.3589,  0.2032]],\n",
       "\n",
       "         [[-0.4836,  0.4204,  1.1629,  ...,  0.4208, -0.0808, -1.0024],\n",
       "          [-0.4373, -0.0435,  0.3944,  ..., -0.1662, -0.1985,  0.1219],\n",
       "          [-0.2621, -0.1597,  0.1489,  ..., -0.0809, -0.3849,  0.1999],\n",
       "          [-0.2815, -0.1909,  0.1588,  ..., -0.1529, -0.3445,  0.2213]],\n",
       "\n",
       "         [[ 0.3481,  0.8732, -0.4171,  ...,  0.1214, -0.6224, -0.7657],\n",
       "          [ 0.3693,  0.4133, -0.0863,  ..., -0.2610, -0.2208, -0.1682],\n",
       "          [ 0.2288,  0.4849,  0.0218,  ..., -0.1596, -0.2921, -0.1492],\n",
       "          [ 0.0761,  0.0870,  0.0537,  ..., -0.0217, -0.1152, -0.2775]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4565,  0.3440, -0.1666,  ...,  0.2881, -1.4450, -0.0399],\n",
       "          [ 0.1780, -0.0540, -0.0328,  ...,  0.4902, -0.9754,  0.1604],\n",
       "          [-0.0157, -0.0162, -0.0935,  ...,  0.4167, -0.2047,  0.3392],\n",
       "          [-0.1792, -0.0550, -0.1336,  ...,  0.3610, -0.1566,  0.2023]],\n",
       "\n",
       "         [[-0.9280,  0.2263, -0.1924,  ...,  0.5600,  0.4704,  0.4140],\n",
       "          [-0.6007,  0.0431, -0.3369,  ...,  0.1602,  0.2264,  0.2187],\n",
       "          [-0.2708, -0.0553, -0.2250,  ..., -0.1635, -0.1490,  0.0195],\n",
       "          [-0.1638, -0.1949, -0.2913,  ...,  0.0022, -0.0896,  0.2785]],\n",
       "\n",
       "         [[ 0.0843,  0.6219, -0.2471,  ..., -1.0867, -0.4793,  0.7211],\n",
       "          [ 0.1750,  0.1657, -0.1813,  ..., -0.5343, -0.2098,  0.3576],\n",
       "          [ 0.1836,  0.0869, -0.1274,  ..., -0.4359, -0.1643, -0.0095],\n",
       "          [ 0.2371, -0.2036,  0.1474,  ..., -0.3046, -0.1781, -0.1269]]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = values.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = values.reshape(batch_size, L, num_heads*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4777,  0.3902,  1.4209,  ..., -1.0867, -0.4793,  0.7211],\n",
       "         [-0.4544, -0.2658, -0.0801,  ..., -0.5343, -0.2098,  0.3576],\n",
       "         [-0.4466, -0.4839, -0.3586,  ..., -0.4359, -0.1643, -0.0095],\n",
       "         [-0.2370, -0.3296, -0.4426,  ..., -0.3046, -0.1781, -0.1269]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled + mask\n",
    "        attention = self.softmax(scaled)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, L, input_dim = x.size()\n",
    "        qkv = self.qkv_layer(x) #1536\n",
    "        qkv = qkv.reshape(batch_size, L, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 5, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 30\n",
    "L = 5\n",
    "x = torch.randn(batch_size,L, input_dim)\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POSITIONAL EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "L = 10\n",
    "d_model = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denominator = torch.pow(10000, even_i/d_model)\n",
    "even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   4.6416,  100.0000, 2154.4343])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_denominator = torch.pow(10000, odd_i/d_model)\n",
    "odd_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(L, dtype = torch.float).reshape(L,1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8415,  0.0464,  0.0022],\n",
       "         [ 0.9093,  0.0927,  0.0043],\n",
       "         [ 0.1411,  0.1388,  0.0065],\n",
       "         [-0.7568,  0.1846,  0.0086],\n",
       "         [-0.9589,  0.2300,  0.0108],\n",
       "         [-0.2794,  0.2749,  0.0129],\n",
       "         [ 0.6570,  0.3192,  0.0151],\n",
       "         [ 0.9894,  0.3629,  0.0172],\n",
       "         [ 0.4121,  0.4057,  0.0194]]),\n",
       " tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "         [ 0.9769,  0.9999,  1.0000],\n",
       "         [ 0.9086,  0.9998,  1.0000],\n",
       "         [ 0.7983,  0.9996,  1.0000],\n",
       "         [ 0.6511,  0.9992,  1.0000],\n",
       "         [ 0.4738,  0.9988,  1.0000],\n",
       "         [ 0.2746,  0.9982,  1.0000],\n",
       "         [ 0.0627,  0.9976,  1.0000],\n",
       "         [-0.1522,  0.9968,  1.0000],\n",
       "         [-0.3599,  0.9960,  1.0000]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE = torch.sin(position/even_denominator)\n",
    "odd_PE = torch.cos(position/odd_denominator)\n",
    "even_PE, odd_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = stacked.reshape(L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.9769,  0.0464,  0.9999,  0.0022,  1.0000],\n",
       "        [ 0.9093,  0.9086,  0.0927,  0.9998,  0.0043,  1.0000],\n",
       "        [ 0.1411,  0.7983,  0.1388,  0.9996,  0.0065,  1.0000],\n",
       "        [-0.7568,  0.6511,  0.1846,  0.9992,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.4738,  0.2300,  0.9988,  0.0108,  1.0000],\n",
       "        [-0.2794,  0.2746,  0.2749,  0.9982,  0.0129,  1.0000],\n",
       "        [ 0.6570,  0.0627,  0.3192,  0.9976,  0.0151,  1.0000],\n",
       "        [ 0.9894, -0.1522,  0.3629,  0.9968,  0.0172,  1.0000],\n",
       "        [ 0.4121, -0.3599,  0.4057,  0.9960,  0.0194,  1.0000]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, L):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.L = L\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, d_model, 2)\n",
    "        odd_i = torch.arange(1, d_model, 2)\n",
    "        even_denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        odd_denominator = torch.pow(10000, odd_i/self.d_model)\n",
    "        position = torch.arange(0,self.L).reshape(self.L, 1)\n",
    "        even_PE = torch.sin(position/even_denominator)\n",
    "        odd_PE = torch.cos(position/odd_denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2).reshape(self.L, self.d_model)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.5522e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "          1.0366e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -3.8347e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "          2.0733e-04,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8103e-01,  2.4509e-01,  ...,  1.0000e+00,\n",
      "          3.1099e-04,  1.0000e+00],\n",
      "        [-7.5680e-01, -7.0591e-01, -6.5717e-01,  ...,  1.0000e+00,\n",
      "          4.1465e-04,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "L = 5\n",
    "PEmodel = PositionalEncoding(d_model, L)\n",
    "PE = PEmodel.forward()\n",
    "print(PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAYER_NORMALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([[[0.2,0.1,0.3],[0.5,0.1,0.1]]])\n",
    "B,S,E = inputs.size()\n",
    "inputs = inputs.reshape(S,B,E)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_shape = inputs.size()[-2:]\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [-(i+1) for i in range(len(parameter_shape))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim= dims, keepdim =True)\n",
    "mean.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2000]],\n",
       "\n",
       "        [[0.2333]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs-mean)**2).mean(dim= dims, keepdim=True)\n",
    "epsilon = 1e-5\n",
    "std = (var + epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs-mean)/std\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma*y + beta\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalisation():\n",
    "    def __init__(self,parameters_shape, eps=1e-5):\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        # dims = [i for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim = dims, keepdim = True)\n",
    "        print(\"mean:\", mean, \"\\n\")\n",
    "        var = ((inputs - mean)**2).mean(dim= dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(\"std: \",std,\"\\n\")\n",
    "        y = (inputs-mean)/std\n",
    "        print(y)\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8991,  2.0463,  0.1889, -0.7382,  0.9180, -1.3116,  0.6804,\n",
       "          -1.0859],\n",
       "         [-1.6766,  2.4427, -0.6603,  1.0465,  0.4836, -1.4417,  0.5144,\n",
       "          -2.2974],\n",
       "         [ 0.7005,  0.7009, -0.7390, -0.7901,  0.0563,  0.3950, -1.2784,\n",
       "           0.3706],\n",
       "         [ 1.2577, -0.2019,  1.7982, -0.2082,  1.8063,  1.0871, -0.3185,\n",
       "           2.4050],\n",
       "         [-1.1118, -0.6608,  0.6784,  0.2754,  1.0659,  1.1715, -0.6905,\n",
       "          -0.1357]],\n",
       "\n",
       "        [[-0.6208,  0.2440, -1.6580, -0.6958,  1.5642, -1.1322, -0.2868,\n",
       "          -1.7205],\n",
       "         [-0.5570,  0.4115, -0.6271,  0.1636, -0.6262,  0.8286,  0.1182,\n",
       "           0.5821],\n",
       "         [ 0.2554, -0.4994,  1.5506,  0.7403,  1.2673,  0.1709, -0.1303,\n",
       "           0.3953],\n",
       "         [-0.5525, -0.2413, -0.7346,  0.4996,  0.0394, -0.5766,  0.7247,\n",
       "           0.1104],\n",
       "         [-0.1518,  0.4539,  0.2824, -0.5254, -0.4645, -2.2145,  0.2749,\n",
       "          -0.4563]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 2\n",
    "S = 5\n",
    "E = 8\n",
    "inputs = torch.randn(B,S,E)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([[[0.0481]]]) \n",
      "\n",
      "std:  tensor([[[0.9884]]]) \n",
      "\n",
      "tensor([[[ 0.8609,  2.0216,  0.1425, -0.7956,  0.8801, -1.3757,  0.6397,\n",
      "          -1.1473],\n",
      "         [-1.7449,  2.4227, -0.7168,  1.0101,  0.4406, -1.5072,  0.4718,\n",
      "          -2.3730],\n",
      "         [ 0.6601,  0.6605, -0.7963, -0.8480,  0.0083,  0.3510, -1.3421,\n",
      "           0.3263],\n",
      "         [ 1.2238, -0.2529,  1.7706, -0.2593,  1.7789,  1.0512, -0.3709,\n",
      "           2.3846],\n",
      "         [-1.1735, -0.7172,  0.6377,  0.2300,  1.0297,  1.1366, -0.7472,\n",
      "          -0.1860]],\n",
      "\n",
      "        [[-0.6768,  0.1982, -1.7262, -0.7526,  1.5339, -1.1942, -0.3388,\n",
      "          -1.7893],\n",
      "         [-0.6122,  0.3677, -0.6831,  0.1168, -0.6822,  0.7897,  0.0710,\n",
      "           0.5403],\n",
      "         [ 0.2098, -0.5539,  1.5202,  0.7003,  1.2335,  0.1242, -0.1805,\n",
      "           0.3513],\n",
      "         [-0.6076, -0.2928, -0.7919,  0.4568, -0.0088, -0.6321,  0.6845,\n",
      "           0.0630],\n",
      "         [-0.2022,  0.4105,  0.2371, -0.5802, -0.5186, -2.2892,  0.2295,\n",
      "          -0.5103]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8609,  2.0216,  0.1425, -0.7956,  0.8801, -1.3757,  0.6397,\n",
       "          -1.1473],\n",
       "         [-1.7449,  2.4227, -0.7168,  1.0101,  0.4406, -1.5072,  0.4718,\n",
       "          -2.3730],\n",
       "         [ 0.6601,  0.6605, -0.7963, -0.8480,  0.0083,  0.3510, -1.3421,\n",
       "           0.3263],\n",
       "         [ 1.2238, -0.2529,  1.7706, -0.2593,  1.7789,  1.0512, -0.3709,\n",
       "           2.3846],\n",
       "         [-1.1735, -0.7172,  0.6377,  0.2300,  1.0297,  1.1366, -0.7472,\n",
       "          -0.1860]],\n",
       "\n",
       "        [[-0.6768,  0.1982, -1.7262, -0.7526,  1.5339, -1.1942, -0.3388,\n",
       "          -1.7893],\n",
       "         [-0.6122,  0.3677, -0.6831,  0.1168, -0.6822,  0.7897,  0.0710,\n",
       "           0.5403],\n",
       "         [ 0.2098, -0.5539,  1.5202,  0.7003,  1.2335,  0.1242, -0.1805,\n",
       "           0.3513],\n",
       "         [-0.6076, -0.2928, -0.7919,  0.4568, -0.0088, -0.6321,  0.6845,\n",
       "           0.0630],\n",
       "         [-0.2022,  0.4105,  0.2371, -0.5802, -0.5186, -2.2892,  0.2295,\n",
       "          -0.5103]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm = LayerNormalisation(inputs.size())\n",
    "layer_norm.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled + mask\n",
    "        attention = self.softmax(scaled)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, L, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x) #1536\n",
    "        qkv = qkv.reshape(batch_size, L, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self,parameters_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean)/std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden,drop_prob):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, hidden)\n",
    "        self.layer2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNormalisation(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNormalisation([d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        residual_x =x\n",
    "        x = self.attention(x, mask=None)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x+residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+residual_x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob =0.1\n",
    "batch_size = 30\n",
    "max_seq_len = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.5165e-02, 7.8972e-02, 9.3994e-01,  ..., 5.4412e-01,\n",
      "          6.7304e-01, 7.0475e-01],\n",
      "         [6.3159e-01, 4.2271e-01, 4.1495e-01,  ..., 8.9289e-01,\n",
      "          3.7069e-01, 7.1196e-01],\n",
      "         [6.3638e-01, 9.4616e-01, 9.3812e-01,  ..., 5.0189e-01,\n",
      "          8.0082e-01, 8.6827e-01],\n",
      "         ...,\n",
      "         [7.9618e-01, 3.9775e-01, 9.7279e-01,  ..., 2.8340e-01,\n",
      "          5.8786e-02, 9.3491e-01],\n",
      "         [7.7949e-01, 2.6950e-01, 3.6036e-01,  ..., 3.7793e-01,\n",
      "          2.3244e-01, 5.1813e-01],\n",
      "         [6.2434e-01, 2.9904e-01, 1.2090e-02,  ..., 4.7482e-02,\n",
      "          1.0576e-01, 9.7183e-01]],\n",
      "\n",
      "        [[8.3025e-02, 2.1868e-01, 7.6788e-01,  ..., 8.6482e-01,\n",
      "          2.1982e-01, 2.8630e-01],\n",
      "         [3.1720e-01, 6.7618e-01, 7.2256e-02,  ..., 1.4309e-02,\n",
      "          8.5812e-01, 7.1306e-01],\n",
      "         [2.9088e-01, 3.8686e-01, 4.7623e-01,  ..., 6.5938e-01,\n",
      "          6.1995e-01, 8.1219e-01],\n",
      "         ...,\n",
      "         [5.2448e-01, 1.8297e-01, 2.1650e-02,  ..., 1.6950e-01,\n",
      "          3.7165e-01, 2.5119e-01],\n",
      "         [2.7520e-01, 4.7777e-02, 7.8710e-01,  ..., 1.2055e-01,\n",
      "          7.9294e-01, 5.0695e-01],\n",
      "         [4.8963e-02, 4.7155e-01, 3.5663e-02,  ..., 6.9346e-01,\n",
      "          2.0626e-01, 9.9963e-01]],\n",
      "\n",
      "        [[5.2078e-01, 9.6775e-01, 3.0394e-01,  ..., 6.4118e-01,\n",
      "          7.1216e-01, 8.7293e-01],\n",
      "         [3.0332e-02, 4.7631e-01, 6.5439e-01,  ..., 6.0256e-01,\n",
      "          5.5580e-01, 5.1832e-01],\n",
      "         [4.9733e-01, 1.2824e-01, 9.1473e-01,  ..., 2.5707e-01,\n",
      "          8.4789e-01, 7.3021e-01],\n",
      "         ...,\n",
      "         [7.7843e-01, 6.9324e-01, 5.3078e-01,  ..., 7.1345e-01,\n",
      "          9.2713e-01, 7.6716e-01],\n",
      "         [2.9437e-01, 6.7515e-01, 9.9915e-01,  ..., 9.2717e-01,\n",
      "          1.4779e-01, 8.7664e-01],\n",
      "         [3.9343e-01, 4.3188e-01, 5.6401e-01,  ..., 8.7154e-01,\n",
      "          4.9271e-01, 3.9577e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[8.6228e-01, 8.1999e-01, 3.1402e-01,  ..., 7.6813e-01,\n",
      "          4.4433e-01, 1.6937e-01],\n",
      "         [7.9240e-01, 7.7880e-01, 9.1069e-01,  ..., 2.3322e-01,\n",
      "          2.4418e-01, 5.5515e-01],\n",
      "         [7.6907e-01, 6.1787e-01, 4.8173e-01,  ..., 7.8007e-01,\n",
      "          1.1146e-01, 3.0362e-01],\n",
      "         ...,\n",
      "         [5.7347e-01, 5.9816e-01, 9.1895e-01,  ..., 5.2979e-01,\n",
      "          5.2279e-01, 1.1550e-01],\n",
      "         [4.6093e-01, 2.2244e-01, 7.9063e-01,  ..., 5.6006e-01,\n",
      "          8.2226e-01, 3.7970e-01],\n",
      "         [9.6633e-01, 6.6164e-01, 1.0971e-01,  ..., 4.6284e-01,\n",
      "          5.5629e-01, 6.1999e-01]],\n",
      "\n",
      "        [[1.5969e-01, 1.0940e-01, 3.3276e-01,  ..., 1.5111e-01,\n",
      "          7.1296e-02, 6.3657e-02],\n",
      "         [8.1217e-01, 3.9662e-01, 5.8944e-01,  ..., 7.1887e-01,\n",
      "          5.0923e-01, 2.8215e-01],\n",
      "         [4.8563e-01, 6.3525e-02, 5.0573e-01,  ..., 7.2733e-02,\n",
      "          4.1497e-01, 5.7441e-01],\n",
      "         ...,\n",
      "         [9.8640e-01, 5.9403e-01, 2.1874e-01,  ..., 6.3222e-01,\n",
      "          4.5126e-01, 9.7167e-01],\n",
      "         [9.6591e-01, 9.3733e-01, 8.1586e-01,  ..., 9.1173e-01,\n",
      "          7.6999e-01, 4.2746e-01],\n",
      "         [7.9821e-01, 8.4728e-01, 5.6028e-01,  ..., 9.4515e-01,\n",
      "          1.6320e-01, 4.1784e-01]],\n",
      "\n",
      "        [[7.6255e-01, 2.8692e-01, 3.5759e-01,  ..., 8.3727e-01,\n",
      "          7.4614e-01, 6.6677e-01],\n",
      "         [5.1433e-01, 6.4269e-01, 4.2329e-01,  ..., 3.7614e-01,\n",
      "          1.8128e-01, 4.2969e-01],\n",
      "         [6.5367e-01, 9.8399e-01, 9.5563e-01,  ..., 8.9005e-01,\n",
      "          8.1696e-01, 4.4715e-01],\n",
      "         ...,\n",
      "         [7.5484e-01, 2.4002e-01, 5.0508e-01,  ..., 3.1236e-01,\n",
      "          4.4089e-01, 3.4493e-01],\n",
      "         [2.5881e-01, 4.2820e-01, 3.4897e-01,  ..., 5.3859e-01,\n",
      "          3.4323e-01, 2.6181e-01],\n",
      "         [3.6730e-01, 1.4190e-01, 9.7123e-03,  ..., 8.1057e-01,\n",
      "          4.7560e-01, 8.2930e-02]]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4750, -1.8187,  1.5631,  ..., -0.3323,  0.8195, -0.3356],\n",
       "         [ 0.1586, -1.1768, -0.2073,  ...,  1.3578,  0.2599,  0.5032],\n",
       "         [ 0.7782,  0.3307,  0.4487,  ...,  0.4960,  1.0771,  1.0387],\n",
       "         ...,\n",
       "         [ 0.6858, -0.6414, -0.1338,  ...,  0.1199, -0.8019,  0.4714],\n",
       "         [ 1.1247, -0.9763, -0.4771,  ..., -0.4158, -0.1121, -1.1807],\n",
       "         [ 0.4370, -0.8591, -0.7835,  ...,  0.4159, -0.9847,  0.7713]],\n",
       "\n",
       "        [[-1.0092, -0.9389, -0.1810,  ...,  1.4506, -0.7547, -1.1111],\n",
       "         [ 0.6898, -0.0060, -1.9131,  ..., -0.1013,  0.8371,  0.7687],\n",
       "         [-1.0107, -0.9244,  0.3137,  ...,  0.2466,  0.4909,  0.3349],\n",
       "         ...,\n",
       "         [-0.0746, -1.7969, -0.9578,  ..., -0.6509,  0.8754, -0.4254],\n",
       "         [-0.7202, -1.6371,  1.1020,  ..., -0.6296,  1.1235,  0.3191],\n",
       "         [-0.8193, -0.8729, -0.1591,  ...,  1.5081, -0.4655,  1.0633]],\n",
       "\n",
       "        [[ 0.8899,  0.2377, -0.4629,  ...,  0.4525,  1.6371,  0.9102],\n",
       "         [-1.5423, -0.6512,  0.5578,  ...,  0.5287, -0.0971, -0.0777],\n",
       "         [-0.9144, -1.8164,  0.5775,  ..., -0.6763,  1.5144,  1.3890],\n",
       "         ...,\n",
       "         [ 0.1720, -0.1930,  1.1443,  ...,  0.3136,  1.6345,  0.9675],\n",
       "         [-1.2584, -0.2412,  1.5785,  ...,  1.3587, -0.3919,  0.7234],\n",
       "         [ 0.9571, -0.4659, -0.3734,  ...,  2.0846, -0.2436, -0.8279]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1953, -0.3471, -0.2063,  ...,  1.0370,  0.3258, -0.5359],\n",
       "         [ 0.8585,  0.2073,  0.0350,  ...,  0.2020, -0.6937,  0.4054],\n",
       "         [ 0.2160, -0.7089,  0.0199,  ...,  1.3662, -0.4020, -0.6294],\n",
       "         ...,\n",
       "         [ 0.3290, -1.0472,  1.3467,  ...,  0.0554, -0.2130, -1.1609],\n",
       "         [-0.9055, -1.8244,  0.7821,  ...,  1.6148,  0.6658, -1.3327],\n",
       "         [ 1.3382, -1.0181, -0.0636,  ...,  0.2186, -0.0258,  0.5768]],\n",
       "\n",
       "        [[-0.1190, -1.4565, -0.9238,  ...,  0.7999, -1.4658, -1.7005],\n",
       "         [ 0.2514, -1.0159, -0.7015,  ...,  0.4689,  0.8737, -0.1391],\n",
       "         [-0.2062, -1.4802,  0.4577,  ..., -0.7816, -0.1577,  0.0167],\n",
       "         ...,\n",
       "         [ 1.6693,  0.2981, -0.8116,  ...,  0.8110,  0.3010,  1.1765],\n",
       "         [ 1.2876,  1.0529,  0.0081,  ...,  1.6497,  0.1490,  0.3812],\n",
       "         [-0.1294, -0.0510,  0.1403,  ...,  1.1205, -1.0048,  0.6818]],\n",
       "\n",
       "        [[ 0.3047, -1.0596, -0.7560,  ...,  1.3662,  1.0114,  0.1963],\n",
       "         [-0.0669, -0.2024, -0.0823,  ...,  0.1447, -0.0832, -0.2392],\n",
       "         [-0.0059,  0.2185,  0.6340,  ...,  1.2833,  0.8204, -0.0460],\n",
       "         ...,\n",
       "         [ 0.2085, -1.4163,  0.2162,  ...,  0.6082,  0.0473, -0.6289],\n",
       "         [-0.8889, -0.5516,  0.3063,  ..., -1.4235, -0.1060, -0.8193],\n",
       "         [-0.3660, -2.0861, -1.1985,  ...,  1.6545,  0.0652, -0.9620]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(d_model, ffn_hidden,num_heads, drop_prob,num_layers)\n",
    "x = torch.rand(batch_size,max_seq_len,d_model)\n",
    "print(x,\"\\n\")\n",
    "encoder.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model, 2*d_model)\n",
    "        self.qlayer = nn.Linear(d_model,d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled1 = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled1 = scaled1 + mask\n",
    "        attention = self.softmax(scaled1)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x,y, mask = None):\n",
    "        batch_size, L, d_model = x.size()\n",
    "        kv = self.kv_layer(x) #1024\n",
    "        q = self.qlayer(y) #512\n",
    "        kv = kv.reshape(batch_size, L, self.num_heads, 2*self.head_dim)\n",
    "        q = q.reshape(batch_size, L, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0,2,1,3)\n",
    "        q = q.permute(0,2,1,3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = scaled + mask\n",
    "        attention = self.softmax(scaled)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, L, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x) #1536\n",
    "        qkv = qkv.reshape(batch_size, L, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden,drop_prob):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, hidden)\n",
    "        self.layer2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self,parameters_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean)/std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model,ffn_hidden, num_heads,  drop_prob):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MultiHeadCrossAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNormalisation(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNormalisation([d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm3 = LayerNormalisation([d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self,x,y,mask):\n",
    "        residual_y = y\n",
    "        y = self.attention(y,mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.norm1(y + residual_y)\n",
    "        residual_y = y\n",
    "        y = self.cross_attention(x,y)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.norm2(y + residual_y)\n",
    "        residual_y = y\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.norm3(y + residual_y)\n",
    "        return y\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x,y,mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self,x,y,mask):\n",
    "        out = self.layers(x,y,mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob =0.1\n",
    "batch_size = 30\n",
    "max_seq_len = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2075, -1.1127,  0.9051,  ..., -0.9427,  0.1867, -0.8870],\n",
      "         [-1.6672, -0.0291, -1.1996,  ...,  0.4924, -0.8352, -0.5981],\n",
      "         [ 0.1387, -1.2652, -0.7215,  ...,  0.8445, -0.4791, -0.0448],\n",
      "         ...,\n",
      "         [-0.1768,  0.3483,  1.1395,  ...,  1.0055,  0.0882,  0.8486],\n",
      "         [ 1.5945, -0.4553, -1.2250,  ...,  0.4515, -0.6192, -0.1373],\n",
      "         [ 0.2959, -0.6728, -2.0257,  ...,  1.3432, -2.4229, -0.8640]],\n",
      "\n",
      "        [[ 0.8897,  0.4753,  0.7557,  ...,  1.5186, -1.5966,  0.1326],\n",
      "         [ 0.4663, -1.8995, -0.3717,  ...,  1.0053, -0.3904, -0.4749],\n",
      "         [-1.3406, -0.7369, -0.5987,  ...,  2.3235, -0.6220,  0.5518],\n",
      "         ...,\n",
      "         [ 0.9946,  0.7019,  0.6131,  ..., -1.1127,  1.2472,  1.0058],\n",
      "         [ 0.2588,  0.4097, -0.4390,  ...,  1.7658,  0.5567,  0.5699],\n",
      "         [ 0.0543,  0.0555, -0.2304,  ..., -1.0518, -0.9701, -1.2106]],\n",
      "\n",
      "        [[ 1.4124,  0.1955,  0.6666,  ...,  0.0695,  0.5957,  0.8089],\n",
      "         [ 1.8126,  0.3701,  0.5399,  ..., -2.0463,  0.2369,  0.1522],\n",
      "         [ 0.3421,  0.1796,  1.2862,  ..., -1.6176, -0.4517,  1.4728],\n",
      "         ...,\n",
      "         [-0.0721, -0.5083,  1.3939,  ...,  1.8873,  0.0520, -1.9709],\n",
      "         [-0.0275,  0.6811,  0.2404,  ...,  0.9613, -0.0161,  0.4777],\n",
      "         [ 0.3690,  0.5749,  1.7737,  ...,  0.1549, -0.1526, -0.6129]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1629,  1.4186, -0.1530,  ..., -0.5949, -0.3189,  0.2655],\n",
      "         [-0.7051,  0.7737, -1.1721,  ...,  0.2383,  1.3826,  1.0508],\n",
      "         [ 1.6857,  0.5020, -0.2194,  ..., -1.0949, -0.1703,  0.4309],\n",
      "         ...,\n",
      "         [-0.6056, -0.2307, -0.9703,  ...,  0.1679, -0.3127, -0.0034],\n",
      "         [-0.5307, -0.0925,  0.8788,  ..., -0.8636, -0.7872, -0.3231],\n",
      "         [ 0.3775,  1.3984,  1.6312,  ..., -0.0298, -0.6716,  0.8015]],\n",
      "\n",
      "        [[ 1.0244, -0.6004, -0.5218,  ...,  1.1433,  1.1662,  1.0566],\n",
      "         [ 1.7065, -1.3247, -0.2164,  ...,  0.5704, -0.6195,  1.8703],\n",
      "         [ 0.2750,  1.9088,  1.6038,  ...,  0.3950,  0.3846, -0.3619],\n",
      "         ...,\n",
      "         [-0.6561,  0.6222,  0.2399,  ..., -1.5485, -0.3951, -0.3124],\n",
      "         [-1.3395,  0.2891,  0.2002,  ..., -1.3145,  0.5720,  0.0113],\n",
      "         [ 0.3919, -1.0455,  0.2870,  ..., -1.8901,  0.0490, -1.6391]],\n",
      "\n",
      "        [[ 0.4461, -0.4056, -1.2748,  ...,  0.1162,  0.5089,  0.2838],\n",
      "         [-0.6670,  1.1715,  1.4036,  ...,  1.1387, -0.3389, -0.6463],\n",
      "         [ 0.0638,  1.4033, -0.0628,  ...,  1.3751,  1.0357,  0.5440],\n",
      "         ...,\n",
      "         [ 0.1860, -0.4010,  0.1531,  ..., -1.5259,  0.2594,  1.1245],\n",
      "         [ 0.4599,  1.4841, -0.1026,  ...,  1.0223,  0.5262,  0.4406],\n",
      "         [-1.0629,  0.0814, -0.3661,  ..., -0.3899,  1.1974,  2.3218]]]) \n",
      "\n",
      "tensor([[[-9.3806e-01, -1.4382e+00,  3.6240e-01,  ...,  1.0419e+00,\n",
      "           9.9012e-01,  4.2507e-01],\n",
      "         [-1.1985e+00, -8.3041e-01,  2.3661e+00,  ..., -2.1860e-01,\n",
      "           3.9479e-01, -1.2838e-01],\n",
      "         [-3.2294e-01,  4.4474e-01,  1.9350e-01,  ...,  5.0468e-01,\n",
      "           4.2463e-01,  7.3892e-01],\n",
      "         ...,\n",
      "         [ 4.4066e-01,  3.9790e-01,  3.2847e-01,  ...,  1.3058e-01,\n",
      "          -3.8671e-01,  6.1231e-01],\n",
      "         [-7.1522e-01,  1.3570e+00, -2.8255e-01,  ..., -7.7056e-01,\n",
      "           8.2251e-01, -5.1643e-01],\n",
      "         [ 1.2824e+00, -5.9187e-01, -1.9600e+00,  ..., -1.3112e+00,\n",
      "           1.5338e-01, -1.1584e+00]],\n",
      "\n",
      "        [[-4.4053e-01,  4.0889e-01,  1.2700e+00,  ..., -3.1569e-01,\n",
      "           9.9980e-01, -1.2543e+00],\n",
      "         [ 1.1014e-01,  7.0561e-02,  1.2010e+00,  ...,  2.3187e+00,\n",
      "          -1.0420e+00, -6.0327e-01],\n",
      "         [ 3.3033e-01, -8.1955e-01,  1.9462e+00,  ...,  2.6603e-04,\n",
      "           6.5418e-01, -6.5433e-01],\n",
      "         ...,\n",
      "         [ 4.7778e-02, -1.9271e-01, -1.7411e+00,  ...,  4.6592e-01,\n",
      "           7.4952e-01,  1.3276e+00],\n",
      "         [ 2.2233e+00,  1.0502e+00,  6.6223e-01,  ...,  1.1056e-03,\n",
      "           1.3384e-01,  1.6425e-01],\n",
      "         [ 7.2616e-01, -8.8836e-01,  1.0583e+00,  ..., -4.5297e-01,\n",
      "           3.9456e-02, -7.9417e-01]],\n",
      "\n",
      "        [[ 5.8638e-02,  2.7485e-01,  5.8872e-01,  ...,  5.8760e-01,\n",
      "          -1.2102e+00,  5.3215e-02],\n",
      "         [ 5.2356e-01,  4.6150e-01,  1.1588e+00,  ...,  8.5346e-01,\n",
      "          -1.4288e+00,  1.4690e+00],\n",
      "         [-1.3680e-01, -1.4225e+00, -5.3583e-02,  ..., -1.1318e+00,\n",
      "          -3.7024e-01, -9.0997e-01],\n",
      "         ...,\n",
      "         [-2.6674e+00, -9.1557e-01,  2.3231e-01,  ...,  1.0281e-02,\n",
      "           1.2331e+00, -6.1779e-01],\n",
      "         [-3.7643e-01,  5.1739e-01, -1.7264e+00,  ..., -1.1250e+00,\n",
      "           1.0688e-01, -1.2727e+00],\n",
      "         [-1.0871e+00, -6.0206e-01, -1.1108e+00,  ..., -2.9577e-01,\n",
      "          -1.7084e+00,  1.2589e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2169e+00,  4.2052e-01,  1.3302e+00,  ...,  4.0641e-01,\n",
      "          -2.9170e-01,  9.9323e-01],\n",
      "         [-6.1624e-01, -1.3657e+00,  3.4707e-01,  ..., -4.4445e-01,\n",
      "           8.0207e-01,  8.4696e-01],\n",
      "         [-1.4287e+00, -1.2562e-01,  1.3999e+00,  ..., -6.3143e-01,\n",
      "          -1.1978e-01, -6.1603e-01],\n",
      "         ...,\n",
      "         [-2.8181e-01,  3.9951e-01,  1.0155e+00,  ...,  7.6916e-01,\n",
      "          -6.8688e-01, -6.2861e-01],\n",
      "         [ 1.0537e+00,  2.8868e-01, -4.8655e-01,  ..., -7.3988e-02,\n",
      "          -3.6725e-01, -8.0158e-01],\n",
      "         [ 4.7074e-02,  1.1196e+00, -1.3722e+00,  ...,  8.7437e-01,\n",
      "           5.6678e-01, -1.1099e+00]],\n",
      "\n",
      "        [[-8.0968e-02, -7.8615e-01, -9.5952e-01,  ..., -8.3319e-01,\n",
      "          -1.5109e+00,  1.0918e+00],\n",
      "         [-5.9642e-02, -2.6510e-02,  7.4232e-01,  ..., -1.3691e+00,\n",
      "          -1.8452e-01,  1.1450e-01],\n",
      "         [-6.9996e-02, -6.8752e-01,  1.9001e+00,  ...,  1.3907e+00,\n",
      "          -1.6826e+00, -1.2631e+00],\n",
      "         ...,\n",
      "         [-2.6262e-01, -8.2751e-01, -3.2109e-01,  ...,  6.3224e-01,\n",
      "           1.3576e-01,  1.0104e+00],\n",
      "         [-7.3295e-01,  2.2033e+00,  5.8215e-01,  ..., -7.0610e-01,\n",
      "           1.2680e+00, -6.8903e-01],\n",
      "         [-8.8351e-01,  7.4357e-01, -2.7283e-01,  ...,  8.9331e-01,\n",
      "          -1.6518e+00,  1.8124e+00]],\n",
      "\n",
      "        [[-1.6495e+00,  1.1685e+00,  2.3400e+00,  ..., -2.0610e-01,\n",
      "           7.7955e-01, -8.6849e-01],\n",
      "         [ 2.5592e-01,  4.3999e-01, -3.8251e-01,  ...,  1.8212e+00,\n",
      "           6.1230e-01,  1.0313e-01],\n",
      "         [ 3.4549e-02, -1.0363e+00,  3.8261e-01,  ..., -6.2024e-01,\n",
      "          -6.5097e-01, -8.2755e-01],\n",
      "         ...,\n",
      "         [ 2.8709e-01,  6.7869e-01,  5.1284e-01,  ..., -8.3524e-01,\n",
      "           2.8946e+00,  1.2706e+00],\n",
      "         [ 2.9515e-02, -8.0803e-02,  1.6144e-01,  ..., -2.6193e-01,\n",
      "           5.1204e-01,  4.0846e-01],\n",
      "         [-4.7009e-01,  3.2452e-01,  3.1737e-01,  ...,  9.1868e-02,\n",
      "           5.1940e-01, -1.8498e+00]]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7983, -0.8367,  0.1433,  ...,  0.9049, -0.4009,  0.4072],\n",
       "         [-0.2109,  0.0369,  1.0698,  ...,  0.3962, -0.5952, -0.3968],\n",
       "         [-0.8200,  0.6144, -0.6390,  ...,  0.4931, -0.6483,  0.2895],\n",
       "         ...,\n",
       "         [ 0.5056,  0.6122, -0.4024,  ..., -0.5370, -0.5390,  0.2195],\n",
       "         [-0.3717,  1.4342,  0.0632,  ..., -0.8621, -0.1562, -0.9854],\n",
       "         [ 0.9549, -0.7390, -0.8214,  ..., -1.1474, -0.6037, -1.2576]],\n",
       "\n",
       "        [[-0.9005,  0.1768,  0.3746,  ..., -0.3882,  1.0456, -1.1290],\n",
       "         [ 0.2105,  0.7157,  1.1502,  ...,  1.5634, -1.5876, -1.5184],\n",
       "         [ 0.0791, -0.9320,  2.4979,  ...,  0.2909, -0.3986, -1.3682],\n",
       "         ...,\n",
       "         [-0.0782, -0.4802, -0.8915,  ...,  0.0947,  0.4195,  0.8013],\n",
       "         [ 0.8723,  1.5424,  0.5017,  ..., -0.8229, -0.8532, -0.7936],\n",
       "         [ 1.0685, -0.9782,  0.7418,  ..., -0.2761, -0.1252, -0.4062]],\n",
       "\n",
       "        [[-0.4163,  0.6572,  3.8089,  ...,  0.8401, -1.4169, -0.0507],\n",
       "         [ 0.2555,  1.6492,  1.8763,  ...,  1.2523, -0.7433,  0.3486],\n",
       "         [-0.1608, -0.4999,  0.3702,  ..., -0.0710, -0.5962, -0.3967],\n",
       "         ...,\n",
       "         [-1.8539, -1.3377,  0.3845,  ...,  0.2051,  0.9783, -0.4004],\n",
       "         [ 0.6923,  1.1251, -2.4213,  ..., -0.9097,  0.4896, -1.0273],\n",
       "         [-0.0088, -1.0736, -0.5935,  ..., -0.1807, -1.8598,  1.9876]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.3651,  0.5593,  1.5062,  ...,  0.7100, -0.2994,  1.8661],\n",
       "         [-0.7511, -0.2591,  0.6230,  ..., -0.5636,  0.9511,  1.5596],\n",
       "         [-0.7997, -0.0336,  0.2908,  ..., -1.4005, -0.0675,  0.3488],\n",
       "         ...,\n",
       "         [-0.3895,  0.5819,  0.4985,  ...,  0.7540, -1.2638, -0.6048],\n",
       "         [ 1.2253, -0.1906, -0.8280,  ..., -0.4373, -1.4665, -1.5930],\n",
       "         [ 0.9075,  1.1408, -1.2520,  ...,  0.9841,  0.7777, -1.2645]],\n",
       "\n",
       "        [[-0.1247,  0.1670, -1.3394,  ..., -0.5642,  0.4887, -0.0308],\n",
       "         [-0.3744, -0.4785, -0.1921,  ..., -0.8546,  1.1830, -0.4021],\n",
       "         [ 0.5827, -0.1155,  1.1824,  ...,  1.3294, -1.2496, -1.3502],\n",
       "         ...,\n",
       "         [ 0.1846, -0.4026,  0.0901,  ...,  0.3747,  0.5710,  1.4184],\n",
       "         [-0.7328,  2.1840,  0.7376,  ..., -0.9439,  0.5127, -1.1986],\n",
       "         [-1.3330,  0.6570, -0.1543,  ...,  1.3313, -1.4949,  2.2148]],\n",
       "\n",
       "        [[-0.1340, -0.8274,  0.3003,  ..., -0.6047, -0.7485, -1.2802],\n",
       "         [ 1.1696, -0.4574, -0.7036,  ...,  1.9961, -0.8886, -0.7323],\n",
       "         [ 1.6169, -0.7109,  0.5961,  ...,  0.0382, -0.5628, -0.4081],\n",
       "         ...,\n",
       "         [ 0.5184,  0.2042,  0.9702,  ..., -0.8516,  2.2909,  0.6832],\n",
       "         [-0.0165,  0.3411,  0.4486,  ...,  0.1031,  0.1682,  1.0355],\n",
       "         [-0.0810, -0.6286,  0.6876,  ...,  0.6447, -0.0771, -1.8310]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(d_model, ffn_hidden,num_heads, drop_prob,num_layers)\n",
    "x = torch.randn(batch_size,max_seq_len,d_model)\n",
    "y = torch.randn(batch_size,max_seq_len,d_model)\n",
    "mask = torch.full([max_seq_len, max_seq_len], float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "print(x,\"\\n\")\n",
    "print(y,\"\\n\")\n",
    "decoder.forward(x,y,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of words: {'', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''}\n"
     ]
    }
   ],
   "source": [
    "consonants = ['', '', '', '', '', '', '', '', '', '', '', \n",
    "              '', '', '', '', '', '', '', '', '', '', '', \n",
    "              '', '', '', '', '', '', '', '', '', '', '']\n",
    "\n",
    "vowels = ['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
    "\n",
    "# Generate all combinations of consonants and vowels\n",
    "word_set = set()\n",
    "for consonant in consonants:\n",
    "    for vowel in vowels:\n",
    "        if vowel:\n",
    "            word_set.add(consonant + vowel)\n",
    "        else:\n",
    "            word_set.add(consonant)\n",
    "\n",
    "print(\"Set of words:\", word_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "PADDING = '<PADDING>'\n",
    "END = '\\n'\n",
    "\n",
    "characters_hindi = [START,'', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '','',\n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ':', '!',\n",
    "                    '?', '(', ')', '', '', '', '', '-', '_',' ', PADDING, END]\n",
    "\n",
    "\n",
    "characters_english = [START,'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "              'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              '.', ',', ':', ';', '!', '?', \"'\", '\"', '(', ')', '[', ']', '{', '}', \n",
    "              '-', '_', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*', '+', '=', '<', '>', \n",
    "              '|', '~', '`', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',' ', PADDING, END]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_hindi = {k:v for k,v in enumerate(characters_hindi)}\n",
    "hindi_to_index = {v:k for k,v in enumerate(characters_hindi)}\n",
    "index_to_english = {k:v for k,v in enumerate(characters_english)}\n",
    "english_to_index = {v:k for k,v in enumerate(characters_english)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\ML\\\\transformer_scratch\\\\en-hi\\\\train.en') as enfile:\n",
    "    english = [next(enfile) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In reply, Pakistan got off to a solid start.\\n',\n",
       " 'The European Union has seven principal decision-making bodies, its institutions: the European Parliament, the European Council, the Council of the European Union, the European Commission, the Court of Justice of the European Union, the European Central Bank and the European Court of Auditors.\\n',\n",
       " 'The Congress leader represents Sivaganga Lok Sabha segment from Tamil Nadu.\\n',\n",
       " 'Prompt the user about connection attempts\\n',\n",
       " 'Further, the Minister announced that Deposit Insurance and Credit Guarantee Corporation (DICGC) has been permitted to increase Deposit Insurance coverage to Rs.\\n',\n",
       " 'Therefore, brothers, be more diligent to make your calling and election sure. For if you do these things, you will never stumble.\\n',\n",
       " 'The review committee meeting chaired by the District Judges will be attended by Collectors, SPs, Superintendents of jails and Secretary of District Legal Services Authority.\\n',\n",
       " 'Police is present on the spot.\\n',\n",
       " 'Prime Minister Narendra Modi is slated to campaign for BJP in second phase of Assembly elections in Udhampur and Poonch district of Jammu and Kashmir tomorrow\\n',\n",
       " 'It has a battery backup of 3050mAh.\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\ML\\\\transformer_scratch\\\\en-hi\\\\train.hi', encoding = 'utf-8') as hifile:\n",
    "    hindi =  [next(hifile) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        .\\n',\n",
       " '       ,  ,   ,         \\n',\n",
       " '          .\\n',\n",
       " '        \\n',\n",
       " '           ()      ,    1        5          \\n',\n",
       " '   ,   ,             ,    ,      \\n',\n",
       " '     , ,            \\n',\n",
       " '    \\n',\n",
       " '                     ()        \\n',\n",
       " ' 3050      \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1172"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.percentile([len(x) for x in english], 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(x) for x in hindi], 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7303\n"
     ]
    }
   ],
   "source": [
    "def is_valid_token(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "                # print(token)\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def isvalidlength(sentence, max_sq_len):\n",
    "    return len(list(sentence))< (max_sq_len-1)\n",
    "\n",
    "valid_index = []\n",
    "for i in range(10000):\n",
    "    hsen, esen = hindi[i], english[i]\n",
    "    if(is_valid_token(hsen,characters_hindi) and is_valid_token(esen, characters_english) and isvalidlength(esen,max_sq_len) and isvalidlength(hsen,max_sq_len)):\n",
    "        valid_index.append(i)\n",
    "print(len(valid_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english, hindi):\n",
    "        self.english = english\n",
    "        self.hindi = hindi\n",
    "    def __len__(self):\n",
    "        return len(self.english)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.english[idx], self.hindi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english,hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize(sentence, language_to_index, start_token=True):\n",
    "    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_indicies.insert(0, language_to_index[START])\n",
    "    # print(len(sentence_word_indicies))\n",
    "    for _ in range(len(sentence_word_indicies), max_sq_len):\n",
    "        sentence_word_indicies.append(language_to_index[PADDING])\n",
    "    return torch.tensor(sentence_word_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_word_indicies = [english_to_index[token] for token in list(english[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(45, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(sentence_word_indicies),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenized, hn_tokenized = [], []\n",
    "for i in valid_index:\n",
    "    eng_sentence, hn_sentence = english[i], hindi[i]\n",
    "    eng_tokenized.append( tokenize(eng_sentence, english_to_index, start_token=False) )\n",
    "    # print( tokenize(eng_sentence, english_to_index, start_token=False) )\n",
    "    hn_tokenized.append( tokenize(hn_sentence, hindi_to_index, start_token=True) )\n",
    "    # print( tokenize(hn_sentence, hindi_to_index, start_token=True) )\n",
    "eng_tokenized = torch.stack(eng_tokenized)\n",
    "hn_tokenized = torch.stack(hn_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 40, 95, 44, 31, 42, 38, 51, 54, 95, 16, 27, 37, 35, 45, 46, 27, 40,\n",
       "        95, 33, 41, 46, 95, 41, 32, 32, 95, 46, 41, 95, 27, 95, 45, 41, 38, 35,\n",
       "        30, 95, 45, 46, 27, 44, 46, 53, 97, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
       "        96, 96])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, hin_batch):\n",
    "    num_sen = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sq_len, max_sq_len],True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "\n",
    "    for i in range(num_sen):\n",
    "        eng_sen_len, hin_sen_len = len(eng_batch[i]), len(hin_batch[i])\n",
    "        eng_padding_char = np.arange(eng_sen_len, max_sq_len)\n",
    "        hin_padding_char = np.arange(hin_sen_len, max_sq_len)\n",
    "        encoder_padding_mask[i,:,eng_padding_char] = True\n",
    "        encoder_padding_mask[i,eng_padding_char, :] = True\n",
    "        decoder_padding_mask_self_attention[i, :, hin_padding_char] = True\n",
    "        decoder_padding_mask_self_attention[i, hin_padding_char, :] = True\n",
    "        decoder_padding_mask_cross_attention[i, :, eng_padding_char] = True\n",
    "        decoder_padding_mask_cross_attention[i, hin_padding_char, :] = True\n",
    "\n",
    "    \n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY,0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "e,d1,d2 = create_masks(eng_tokenized,hn_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.ones(1,200),(torch.matmul(e[0],torch.ones((200,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151,\n",
       "       152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
       "       165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177,\n",
       "       178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190,\n",
       "       191, 192, 193, 194, 195, 196, 197, 198, 199])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(100, max_sq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  ...,  True,  True,  True],\n",
       "        [False, False,  True,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False,  True,  True],\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.full([max_sq_len, max_sq_len],True),diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, max_sq_len, d_model, language_to_index, START, END, PADDING):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sq_len = max_sq_len\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sq_len)\n",
    "        self.dropout = nn.Droupout(p=0.1)\n",
    "        self.START = START\n",
    "        self.END = END\n",
    "        self.PADDING = PADDING\n",
    "    \n",
    "    def batch_tokenisation(self, batch, start_token = True, end_token= True):\n",
    "        def tokenize(sentence, language_to_index, start_token=True):\n",
    "            sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, language_to_index[START])\n",
    "            # print(len(sentence_word_indicies))\n",
    "            for _ in range(len(sentence_word_indicies), max_sq_len):\n",
    "                sentence_word_indicies.append(language_to_index[PADDING])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "        eng_tokenized, hn_tokenized = [], []\n",
    "        for i in valid_index:\n",
    "            eng_sentence, hn_sentence = english[i], hindi[i]\n",
    "            eng_tokenized.append( tokenize(eng_sentence, english_to_index, start_token=False) )\n",
    "            # print( tokenize(eng_sentence, english_to_index, start_token=False) )\n",
    "            hn_tokenized.append( tokenize(hn_sentence, hindi_to_index, start_token=True) )\n",
    "            # print( tokenize(hn_sentence, hindi_to_index, start_token=True) )\n",
    "        eng_tokenized = torch.stack(eng_tokenized)\n",
    "        hn_tokenized = torch.stack(hn_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, L):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.L = L\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, d_model, 2).float()\n",
    "        # odd_i = torch.arange(1, d_model, 2).float()\n",
    "        even_denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        # odd_denominator = torch.pow(10000, odd_i/self.d_model)\n",
    "        position = torch.arange(self.L).reshape(self.L, 1)\n",
    "        even_PE = torch.sin(position/even_denominator)\n",
    "        odd_PE = torch.cos(position/even_denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2).reshape(self.L, self.d_model)\n",
    "        # print(\"Positional Encoding\")\n",
    "        return stacked\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, max_sq_len, d_model, language_to_index, START, END, PADDING):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sq_len = max_sq_len\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sq_len)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START = START\n",
    "        self.END = END\n",
    "        self.PADDING = PADDING\n",
    "    \n",
    "    def batch_tokenisation(self, batch, start_token = True, end_token= True):\n",
    "        def tokenize(sentence, start_token=True, end_token= True):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[START])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sq_len):\n",
    "                sentence_word_indicies.append(self.language_to_index[PADDING])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "        tokenized = []\n",
    "        for i in batch:\n",
    "            tokenized.append( tokenize(i, start_token=start_token, end_token = end_token) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized\n",
    "\n",
    "    def forward(self, x, start_token, end_token): # sentence\n",
    "        x  = self.batch_tokenisation(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        # print(\"SentenceEmbedding\")\n",
    "        return x\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model, 2*d_model)\n",
    "        self.qlayer = nn.Linear(d_model,d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled1 = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled1 = (scaled1.permute(1,0,2,3) + mask).permute(1,0,2,3)\n",
    "        attention = self.softmax(scaled1)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x,y, mask = None):\n",
    "        batch_size, L, d_model = x.size()\n",
    "        kv = self.kv_layer(x) #1024\n",
    "        q = self.qlayer(y) #512\n",
    "        kv = kv.reshape(batch_size, L, self.num_heads, 2*self.head_dim)\n",
    "        q = q.reshape(batch_size, L, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0,2,1,3)\n",
    "        q = q.permute(0,2,1,3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        # print(\"MultiHeadCrossAttention\")\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3*d_model)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self,q,k,v,mask=None):\n",
    "        d_k = q.shape[-1]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        scaled = torch.matmul(q, k_t) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled = (scaled.permute(1,0,2,3) + mask).permute(1,0,2,3)\n",
    "        attention = self.softmax(scaled)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, L, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x) #1536\n",
    "        qkv = qkv.reshape(batch_size, L, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        out, attention = self.scaled_dot_product_attention(q,k,v,mask)\n",
    "        out = out.permute(0,2,1,3)\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        out = self.linear(out)\n",
    "        # print(\"MultiHeadAttention\")\n",
    "        return out\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden,drop_prob):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, hidden)\n",
    "        self.layer2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        # print(\"PositionWiseFeedForward\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self,parameters_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean)**2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean)/std\n",
    "        out = self.gamma * y + self.beta\n",
    "        # print(\"LayerNormalisation\")\n",
    "        return out\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model,ffn_hidden, num_heads,  drop_prob):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MultiHeadCrossAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNormalisation(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNormalisation([d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm3 = LayerNormalisation([d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self,x,y,self_attention_mask, cross_attention_mask):\n",
    "        residual_y = y\n",
    "        y = self.attention(y, self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.norm1(y + residual_y)\n",
    "        residual_y = y\n",
    "        y = self.cross_attention(x,y, cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.norm2(y + residual_y)\n",
    "        residual_y = y\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.norm3(y + residual_y)\n",
    "        return y\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    # def forward(self, *inputs):\n",
    "    def forward(self, x,y, self_mask, cross_mask):\n",
    "        # x,y,self_mask, cross_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,self_mask, cross_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, language_to_index, START, END, PADDING):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sq_len, d_model, language_to_index, START, END, PADDING)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self,x,y,self_attention_mask, cross_attention_mask, START, END):\n",
    "        y = self.sentence_embedding(y,START, END)\n",
    "        # try:\n",
    "        #     y = self.layers(x,y,self_attention_mask, cross_attention_mask)\n",
    "        # except:\n",
    "        #     print(\"Error is in decoder\")\n",
    "        y = self.layers(x,y,self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNormalisation(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p = drop_prob)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, ffn_hidden, drop_prob)\n",
    "        self.norm2 = LayerNormalisation([d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self,x, self_attention_mask):\n",
    "        residual_x =x\n",
    "        x = self.attention(x, mask= self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x+residual_x)\n",
    "        residual_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+residual_x)\n",
    "        return x\n",
    "\n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    # def forward(self, *inputs):\n",
    "    def forward(self, x, self_mask):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_mask)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, language_to_index, START, END, PADDING):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sq_len, d_model, language_to_index, START, END, PADDING)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self,x, self_attention_mask, START, END):\n",
    "        x = self.sentence_embedding(x, START, END)\n",
    "        # try:\n",
    "        #     x = self.layers(x, self_attention_mask)\n",
    "        # except:\n",
    "        #     print(\"Error is in encoder\")\n",
    "        # return x\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class  Transformer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, hindi_vocab_size, english_to_index, hindi_to_index, START, END, PADDING):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ffn_hidden = ffn_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, english_to_index, START, END, PADDING)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, hindi_to_index, START, END, PADDING)\n",
    "        self.linear = nn.Linear(d_model, hindi_vocab_size)\n",
    "    \n",
    "    def forward(self, x, y,encoder_self_attention_mask = None, decoder_self_attention_mask = None, decoder_cross_attention_mask = None, enc_start = False, dec_start= False, dec_end = False, enc_end = False):\n",
    "        x = self.encoder(x, encoder_self_attention_mask, enc_start, enc_end)\n",
    "        out = self.decoder(x, y , decoder_self_attention_mask, decoder_cross_attention_mask, dec_start, dec_end)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<START>'\n",
    "PADDING = '<PADDING>'\n",
    "END = '<END>'\n",
    "\n",
    "characters_hindi = [START,'', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '','',\n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '', '', '', '', \n",
    "                    '', '', '', '', '', '', '', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ':', '!',\n",
    "                    '?', '(', ')', '', '', '', '', '-', '_',' ', PADDING, END]\n",
    "\n",
    "\n",
    "characters_english = [START,'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "              'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "              'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "              'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "              '.', ',', ':', ';', '!', '?', \"'\", '\"', '(', ')', '[', ']', '{', '}', \n",
    "              '-', '_', '/', '\\\\', '@', '#', '$', '%', '^', '&','|', '~', '`', \n",
    "              '0','1', '2', '3', '4', '5', '6', '7', '8', '9',' ', PADDING, END]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vocab_size = len(characters_hindi)\n",
    "enlish_vocab_size = len(characters_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\ML\\\\transformer_scratch\\\\en-hi\\\\train.en') as enfile:\n",
    "    english = [next(enfile) for _ in range(10000)]\n",
    "with open('D:\\\\ML\\\\transformer_scratch\\\\en-hi\\\\train.hi', encoding = 'utf-8') as hifile:\n",
    "    hindi =  [next(hifile) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_hindi = {k:v for k,v in enumerate(characters_hindi)}\n",
    "hindi_to_index = {v:k for k,v in enumerate(characters_hindi)}\n",
    "index_to_english = {k:v for k,v in enumerate(characters_english)}\n",
    "english_to_index = {v:k for k,v in enumerate(characters_english)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7310\n"
     ]
    }
   ],
   "source": [
    "def is_valid_token(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def isvalidlength(sentence, max_sq_len):\n",
    "    return len(list(sentence))< (max_sq_len-1)\n",
    "\n",
    "valid_index = []\n",
    "for i in range(10000):\n",
    "    if hindi[i][-1]=='\\n':\n",
    "            hindi[i] = hindi[i][:-1]\n",
    "    if english[i][-1]=='\\n':\n",
    "        english[i] = english[i][:-1]\n",
    "    hsen, esen = hindi[i], english[i]\n",
    "    if(is_valid_token(hsen,characters_hindi) and is_valid_token(esen, characters_english) and isvalidlength(esen,max_sq_len) and isvalidlength(hsen,max_sq_len)):\n",
    "        valid_index.append(i)\n",
    "print(len(valid_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "d_model = 512\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob =0.1\n",
    "num_layers = 1\n",
    "max_sq_len = 200\n",
    "hindi_vocab_size = len(characters_hindi)\n",
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sq_len, hindi_vocab_size, english_to_index, hindi_to_index, START, END, PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(93, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalisation()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionWiseFeedForward(\n",
       "          (layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalisation()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(101, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (qlayer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalisation()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionWiseFeedForward(\n",
       "          (layer1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (layer2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalisation()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (norm3): LayerNormalisation()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, hindi_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.hindi_sentences = hindi_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.hindi_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = [english[i] for i in valid_index]\n",
    "hindi_sentences = [hindi[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences, hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Congress leader represents Sivaganga Lok Sabha segment from Tamil Nadu.',\n",
       " '          .')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In reply, Pakistan got off to a solid start.', 'The Congress leader represents Sivaganga Lok Sabha segment from Tamil Nadu.', 'Prompt the user about connection attempts', 'Further, the Minister announced that Deposit Insurance and Credit Guarantee Corporation (DICGC) has been permitted to increase Deposit Insurance coverage to Rs.', 'Therefore, brothers, be more diligent to make your calling and election sure. For if you do these things, you will never stumble.', 'The review committee meeting chaired by the District Judges will be attended by Collectors, SPs, Superintendents of jails and Secretary of District Legal Services Authority.', 'Police is present on the spot.', 'Prime Minister Narendra Modi is slated to campaign for BJP in second phase of Assembly elections in Udhampur and Poonch district of Jammu and Kashmir tomorrow', 'It has a battery backup of 3050mAh.', 'All 176 passengers died in the incident.', 'Not invited', 'Sunny Leone plays a paranormal expert in the series.', 'Then after it becomes dry, rinse off with water.', 'She is also the national spokesperson of BJP.', 'By this only, we can build our country and make it great again.', 'Coconut oil and lemon juice pack:', 'About 55 lakh tonnes of foodgrains are required monthly under the National Food Security Act (NFSA) and other welfare schemes.', 'All our life as well as all our essential being is transformed into the possession of Sachchidananda.', 'The whole world is coming here.', 'The party has fielded Gananan Balachandra Mangasuli from Athani, Bharamgouda Alagouda Kage from Kagwad, and Lakhan Jarkiholi from Gokak Assembly constituencies.', 'The minimum temperature was at 2 degrees Celsius.', 'Small scale industries have large employment potential with small investment.', 'At Rajiv Gandhi Super Specialty Hospital, the death rate was 6% in early June and 7% in early July.', 'Biotechnology, along with rapid advances in engineering and informatics, is radically changing some of the fundamental concepts regarding living organisms.', 'Congress has won most of these seats.', 'New Delhi, 14th May, 2010', 'Several agreements are expected to be signed after the talks.', 'Algeria military plane crash', 'up and down', 'Then they came to meet me.'), ('        .', '          .', '        ', '           ()      ,    1        5          ', '   ,   ,             ,    ,      ', '     , ,            ', '    ', '                     ()        ', ' 3050      ', '     176      .', '  ', '         ', '            ', '       ', '            ', '     :', '        ()            55       ', '          -   ', '     ', '      ,                 ', '    2     ', '-           ', '                          ', '         ', '          ', ' , 14 , 2010', '         ', '     ', '-', '   ')]\n",
      "[('Probably a lot of you know the story of the two salesmen who went down to Africa in the 1900s.', 'In regard to the Census 2021, the activities required to be undertaken in 2018 were discussed.', 'I wont comment on it.', 'Tejashwi slams Nitish government', 'Was he not a terrorist?', 'Zenan, and Hadashah, and Migdal-gad,', 'This increases borrowing.', 'The hand tried the knob again.', 'Sleep is very important in order to be healthy.', 'Friends, when we talk about Planet, you are also observing that today ISA, i.e. International Solar Alliance is becoming a big global movement.', 'Bollywood Celebrities like Salman Khan, Shilpa Shetty Kundra, R Madhavan and others were present at the event.', 'That was nice of him, Cahill said.', '\"Speaking to IANS, Vijendra Singh, Karni Sena spokesperson, said: \"\"Rajputs can never even think of attacking a school bus.\"', 'Police reached the spot and took up investigation.', '\"No child of God should ever suffer such horror,\"\" Trump said.\"', 'But all was not over.', 'What is JCPOA?', 'Anushka Sharma and Virat Kohli celebrated their first Diwali together post their wedding.', 'That year, a party of Kolkar nearly caught Raja but he escaped thanks to timely warning of a Kurumba guard.', 'Due to this, he got injuries.', 'The fate of 15 candidates would be decided tomorrow.', \"Mumbai, June 5(ANI): It's official\", 'Add onions, green chillies, coriander, ginger and garlic.', 'Odisha Chief Minister Naveen Patnaik held discussion with ArcelorMittal Group Chairman Lakshmi Mittal on investment in the state via video conferencing.', 'Make exercise a part of your daily routine.', 'People started gathering.', 'This, in a way, is one of the challenges of how to interpret a really good idea.', 'So, temperature does not know who is good and who is bad right.', 'The films premise is both interesting and exciting.', 'Director: Shyam Benegal'), ('       ()          .', ' 2021    2018           ', '     ', '      ', '    ', ' , , ,', '    ', '       ', '          ', ',         ,          ,           ', '   ,  ,  , ,         ', '  ,   ', '       ,               ', '      ', '                 ', '      .', '  ?', '                        ', ' ,            ,             ', '    ', '     15       ', ', 19  ()', ', ,  ,      ', '                       ', '          ', '     ', '              .', ',           ', '       ', ' :  ')]\n",
      "[('Obtain loans from a bank or an individual.', 'Prime Minister Modi said the Supreme Court verdict on Ayodhya will not be matter of victory or loss for anyone.', 'are doing.', 'The next thing happening is Mr. India sitting next to me and I am working with him.', 'The rath [vehicle] will be modeled on the proposed Ram temple and is being prepared in Mumbai, said Anil Kumar Singh, Muslim Rashtriya Manch coordinator for the Awadh region.', \"Recently, Ronaldo equalled Josef Bican's all-time highest goal record of 759 strikes.\", 'He was later taken to a hospital and treated.', 'The main occupation of the people of Mota Vadala village is agriculture, farming, animal husbandry and diamond industry.', 'They should stay away from the technology and spend more time studying.', 'People are in distress due to skyrocketing prices of diesel and petrol.', 'The act of avoiding the liability of taxation.', 'So thats a good thing.', 'Rajeev Goyal will be appointed as CFO of AFS.', 'He had sustained the shoulder injury during his short stint in the Indian Premier League (IPL).', 'India is a member of the FATF as well as the APG.', 'Really, there is no reason for us to worry about anything.', 'The Ministry said, overall 766 routes have been sanctioned so far under the UDAN scheme.', 'but those who disbelieve in the Everlasting Life deviate from the Path.', 'There is high prevalence of Anaemia and other ailments, especially among women and adolescent girls.', 'The owner of the unidentified vehicle fled from the spot after the accident.', 'The Prime Minister, Shri Narendra Modi interacted with Karnataka Chief Minister, Shri B S Yediyurappa on the rainfall and flood situation in various parts of Karnataka.', \"Volunteers don 't get paid, not because they' re worthless, but because they 're priceless. - Sherry Anderson\", 'But before jumping into such issues, you might say, Do you mind if I ask about...', 'He screamed.', 'He is at present lodged in a prison in London.', 'Considering that crop segment constitutes a dominant component of the GVA computation, its performance is very critical.', 'In both conditions your doctor can prescribe you some analgesics or other medicines, or he can refer you to another specialist or surgeon.', 'Data integrity and security are pressing issues for electronic commerce.', 'He repeated this assertion several times.', 'With everyones cooperation we will soon defeat Corona completely in the state, he said.'), ('         .', '     ,         ,    -  ', '  .', '                    ', '            ,               .', '       759       ', '          ', '        , ,     ', '           .', '-         ', '      ', '    ', '        ', '           ', '       ', ',                 ', '                 766      ', '               ', '             ', '              ', '                          ', '              ,        . - ', '     ', '  ', '          .', '              ,     ', '                                    .', '             ', '        ', '   ,        ')]\n",
      "[('She shared pictures on her Instagram stories.', 'FCRA Comes into Force W.E.F May 2011', 'On Wednesday morning, Sonu Singh, Devendra Singh and Abhishek Singh came to the village.', 'The insect life at high elevations is also remarkable for the pronounced predominance of wingless and flightless forms.', \"There's no doubt he's the best.\", 'Later this report was also circulated widely in social media.', 'Many Egyptian and Roman slaves were harshly exploited.', 'The primary treatment is epinephrine.', 'Quality Time', \"You don 't say?\", 'However, in each case, Jehovahs discipline is motivated by love, and its goal is to benefit the recipient.', 'Malaysia is the worlds second-largest producer of palm oil after Indonesia.', 'Whereas, the moment you got the moment you got that high acid soluble and heat stable proteins, which are responsible for the foaming of the milk.', 'WHAT would we do without sugar?', 'Every time she has to pass him, he and his friends grope her.', 'Woman dies during childbirth', \"China has opposed India's bid to get an NSG membership saying India was yet to sign the NPT.\", 'The house is painted which means that people were living there for long.', 'The meeting was also attended by six Shiv Sena ministers.', 'I will keep on coming.', 'A police investigation into the matter is currently underway.', 'Thereafter, his condition began to deteriorate.', 'He was residing in a rented house there.', 'Her family and relatives were informed about the incident.', 'This would help reduce the price.', 'In this movie, she was seen along with Sushant Singh Rajput.', 'I love you so dearly that even if you are dead, you will be alive to me.', 'Everything else is an excuse.', 'The rainfall has affected road and rail traffic.', 'If you pull a knife, you have to use ii.'), ('           ', ' 2011   ', '        ,            ', '-                      .', ' :  ', '              ', '             ', '    ', ' ', ' !', '        ,         ', '             ', ',      ,          ,        ', '     ?', '                 ', '     ', '             ,    (  )     ', '                 ', '         .', '    ', '        ', '        .', '        ', '           .', '         ', '         .', '                  ', '   .', '            ', '  ,    .')]\n",
      "[('The law and order situation in the state is abysmal.', 'A special CBI court in Ranchi on Monday convicted RJD chief and former Bihar chief minister Lalu Prasad Yadav in the fourth case of fodder scam in which he figured as an accused.', \"Aayush Sharma is married to Salman's sister Arpita Khan.\", 'Nearby roads have been cordoned off.', 'And when mankind are gathered (on the Day of Resurrection), they (false deities) will become enemies for them and will deny their worshipping.', 'Satellite image shows China is building a dam on the Galwan river', '\"May this festival light up our lives and usher in peace, prosperity and happiness in our lives.\"\"\"', 'Get out', 'But you box like one.', 'The Minister informed that millets include Jowar, Bajra, Ragi, little millets include Kutki, Kodo, Sawa, Kangni and Cheena', 'In India, they have dressed Kareena Kapoor Khan, Sonakshi Sinha, Deepika Padukone, Katrina Kaif, Priyanka Chopra to name a few.', 'During the raid they seized several documents.', 'And when you look inside of it, you have 7 plus 3 times 3.', 'Karnataka were the defending champions.', 'India beat Afghanistan by 11 runs', 'He published the news in the newspaper.', 'To update the mobile number', 'I am still a long way from achieving that but I am trying.', 'The minister said this', 'After this, he was removed.', 'Priced from Rs 40.', 'There is no response from the government yet.', 'The movie was written by Salim Khan and Javed Akhtar.', 'India is, and will remain, sensitive to Bhutan and its interests.', 'He informed that electronics skill development programme for youths have also been started.', 'Shri Bhakta was a veteran Parliamentarian who understood the pulse of the people.', 'The police is investigating his death.', 'The work is arranged according to the months.', 'Prime Minister Narendra Modi extended greetings to President on his birthday.', '\"Reacting on issues of shortage of medical supplies in Kashmir, Raveesh Kumar said, \"\"Not even in one incident any hospital has reported shortage of drug or of any disposable item.\"'), ('          ', '             ', '            ', '       ', '                ', '              ', '          ,      ', '  ', '    .', ', , ,     , , ', ' ,  ,  ,                  ', '        ', '      ,  7  3 3 ', '   ', '       11   ', '      ', '     ', '          ,      ', '    ', '      .', ' 40  ', '           ', ' -               ', '          ', '                ', '            ', '         .', '       ', '             ', '                      ')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=hindi_to_index[PADDING],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, hin_batch):\n",
    "    num_sen = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sq_len, max_sq_len],True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sen, max_sq_len, max_sq_len], False)\n",
    "\n",
    "    for i in range(num_sen):\n",
    "        eng_sen_len, hin_sen_len = len(eng_batch[i]), len(hin_batch[i])\n",
    "        eng_padding_char = np.arange(eng_sen_len, max_sq_len)\n",
    "        hin_padding_char = np.arange(hin_sen_len, max_sq_len)\n",
    "        encoder_padding_mask[i,:,eng_padding_char] = True\n",
    "        encoder_padding_mask[i,eng_padding_char, :] = True\n",
    "        decoder_padding_mask_self_attention[i, :, hin_padding_char] = True\n",
    "        decoder_padding_mask_self_attention[i, hin_padding_char, :] = True\n",
    "        decoder_padding_mask_cross_attention[i, :, eng_padding_char] = True\n",
    "        decoder_padding_mask_cross_attention[i, hin_padding_char, :] = True\n",
    "\n",
    "    \n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY,0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 5.7827839851379395\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction: ?6--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:18, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('?              --------                                                                                                                              ',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [07:45,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 2.044405460357666\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:51,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [1:51:25,  4.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.3778045177459717\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [1:51:33,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [1:54:30, 28.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.25203636288642883\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:48,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.07739191502332687\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [15:53,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.05354005843400955\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [19:03,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.046221617609262466\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:42,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.02154639922082424\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [15:14,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.014770996756851673\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [18:25,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.01832226850092411\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:35,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.007277653086930513\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [15:09,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.006264945026487112\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [18:16,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.010343718342483044\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:20,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0037759076803922653\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [14:33,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.004223404452204704\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [17:37,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.004640422761440277\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:18,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0038888873532414436\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [14:20,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.0025137884076684713\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [16:59,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.003547125495970249\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [06:39,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0013886024244129658\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [06:43,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [13:06,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.002567446092143655\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [13:10,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [15:52,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.0025121988728642464\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [08:08,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0015465759206563234\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [14:30,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.0015061594313010573\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [14:34,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [17:04,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.0014985327143222094\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [05:56,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0008587213233113289\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [11:57,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.0011369973653927445\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [14:31,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 0.001530498848296702\n",
      "English: In reply, Pakistan got off to a solid start.\n",
      "Hindi Translation:         .\n",
      "Hindi Prediction:         .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [06:01,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 : 0.0006300988607108593\n",
      "English: People worship Goddess Lakshmi on this day.\n",
      "Hindi Translation:        \n",
      "Hindi Prediction:        \n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [11:53,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200 : 0.0008362542721442878\n",
      "English: The quicker this is done the better for the country.\n",
      "Hindi Translation:     ,       .\n",
      "Hindi Prediction:     ,       .\n",
      "Evaluation translation (What is your name?) : ('<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [14:21,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in tqdm(enumerate(iterator)):\n",
    "        transformer.train()\n",
    "        eng_batch, hn_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, hn_batch)\n",
    "        optim.zero_grad()\n",
    "        hn_predictions = transformer(eng_batch, hn_batch, encoder_self_attention_mask.to(device), decoder_self_attention_mask.to(device), decoder_cross_attention_mask.to(device),False,False,True,True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenisation(hn_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(hn_predictions.view(-1, hindi_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == hindi_to_index[PADDING], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Hindi Translation: {hn_batch[0]}\")\n",
    "            hn_sentence_predicted = torch.argmax(hn_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in hn_sentence_predicted:\n",
    "              if idx == hindi_to_index[END]:\n",
    "                break\n",
    "              predicted_sentence += index_to_hindi[idx.item()]\n",
    "            print(f\"Hindi Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            hn_sentence = (\"\",)\n",
    "            eng_sentence = (\"What is your name?\",)\n",
    "            for word_counter in range(max_sq_len):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, hn_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          hn_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),False,False,True,False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_hindi[next_token_index]\n",
    "                hn_sentence = (hn_sentence[0] + next_token, )\n",
    "                if next_token == END:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (What is your name?) : {hn_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_counter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_sq_len):\n\u001b[0;32m      3\u001b[0m     encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\u001b[38;5;241m=\u001b[39m create_masks(eng_sentence, hn_sentence)\n\u001b[1;32m----> 4\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mhn_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     next_token_prob_distribution \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m0\u001b[39m][word_counter] \u001b[38;5;66;03m# not actual probs\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     next_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_prob_distribution)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 276\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start, dec_start, dec_end, enc_end)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y,encoder_self_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, decoder_self_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, enc_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, dec_start\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, dec_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, enc_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, encoder_self_attention_mask, enc_start, enc_end)\n\u001b[1;32m--> 276\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 210\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, y, self_attention_mask, cross_attention_mask, START, END)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,y,self_attention_mask, cross_attention_mask, START, END):\n\u001b[1;32m--> 210\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSTART\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m#     y = self.layers(x,y,self_attention_mask, cross_attention_mask)\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m#     print(\"Error is in decoder\")\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x,y,self_attention_mask, cross_attention_mask)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     x  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_tokenisation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     57\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m, in \u001b[0;36mSentenceEmbedding.batch_tokenisation\u001b[1;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[0;32m     48\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m---> 50\u001b[0m     tokenized\u001b[38;5;241m.\u001b[39mappend( \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[0;32m     51\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(tokenized)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mSentenceEmbedding.batch_tokenisation.<locals>.tokenize\u001b[1;34m(sentence, start_token, end_token)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(sentence, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 40\u001b[0m     sentence_word_indicies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sentence)]\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_token:\n\u001b[0;32m     42\u001b[0m         sentence_word_indicies\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_to_index[START])\n",
      "\u001b[1;31mKeyError\u001b[0m: '<'"
     ]
    }
   ],
   "source": [
    "eng_sentence = (\"good\",)\n",
    "for word_counter in range(max_sq_len):\n",
    "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, hn_sentence)\n",
    "    predictions = transformer(eng_sentence,\n",
    "                                hn_sentence,\n",
    "                                encoder_self_attention_mask.to(device), \n",
    "                                decoder_self_attention_mask.to(device), \n",
    "                                decoder_cross_attention_mask.to(device),False,False,True,False)\n",
    "    next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "    next_token = index_to_hindi[next_token_index]\n",
    "    hn_sentence = (hn_sentence[0] + next_token, )\n",
    "    if next_token == END:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
